{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eiH6PmNWnKOU",
        "outputId": "5574370c-e7fe-4c29-8a3d-ba6a377778ad"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import bfloat16\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    pipeline,\n",
        "    GenerationConfig,\n",
        "    BitsAndBytesConfig,\n",
        ")\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
        "from rich import print as rprint\n",
        "from rich.panel import Panel\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JObqP13JnKOX",
        "outputId": "6be6a523-e049-4ebe-d230-dd3387f62755"
      },
      "outputs": [],
      "source": [
        "device = 'cpu'\n",
        "if(torch.cuda.is_available()):\n",
        "    device = 'cuda'\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0sam-l3nKOb",
        "outputId": "c261f032-4add-4b7f-d026-eac17724d7fb"
      },
      "outputs": [],
      "source": [
        "loader = TextLoader(\"/Pytorch_Codes/BD Police.txt\", encoding=\"utf8\")\n",
        "texts = loader.load()\n",
        "\n",
        "character_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
        "texts = character_splitter.split_documents(texts)\n",
        "\n",
        "print(f\"Number of chunks: {len(texts)}\")\n",
        "print(\"Document created successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DI-ODw-JnKOc",
        "outputId": "f74c426d-91ef-478e-9597-7452f016c1d4"
      },
      "outputs": [],
      "source": [
        "documents = [doc.page_content for doc in texts]\n",
        "len(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRbcqesLnKOd",
        "outputId": "b0678a9d-f8ef-4385-fc95-4b3fe5369ffe"
      },
      "outputs": [],
      "source": [
        "model_kwargs = {\"device\": device}\n",
        "embed_model_id = \"BAAI/bge-m3\"\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=embed_model_id, model_kwargs = model_kwargs)\n",
        "print(f\"Embedding Model: {embed_model_id} has been loaded!\")\n",
        "\n",
        "db = Chroma.from_texts(texts=documents, embedding=embeddings, persist_directory=\"chroma_db\")\n",
        "print(\"Chroma database updated successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0RVoJyynKOf"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"[INST]\n",
        "<>\n",
        "You are a helpful Bangla AI assistant.\n",
        "\n",
        "Use the following pieces of 'context' to answer the user's questions. Only Respond in Bangla.\n",
        "\n",
        "Context:\n",
        "    {context}\n",
        "\n",
        "Question: {question}[/INST]\n",
        "Helpful Answer (in Bangla):\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate(\n",
        "    input_variables=[\"question\", \"context\"],\n",
        "    output_parser=None,\n",
        "    partial_variables={},\n",
        "    messages=[\n",
        "        HumanMessagePromptTemplate(\n",
        "            prompt=PromptTemplate(\n",
        "                input_variables=[\"question\", \"context\"],\n",
        "                output_parser=None,\n",
        "                partial_variables={},\n",
        "                template=template,\n",
        "                template_format=\"f-string\",\n",
        "                validate_template=True,\n",
        "            ),\n",
        "            additional_kwargs={},\n",
        "        )\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "La1LZpAmnKOg"
      },
      "outputs": [],
      "source": [
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWn0DtcOnKOh"
      },
      "outputs": [],
      "source": [
        "\n",
        "api_key = \"\"  # Groq API Key\n",
        "groq_chat = ChatGroq(\n",
        "            groq_api_key=api_key,\n",
        "            model_name='llama-3.1-8b-instant' # OR 'gemma2-9b-it'\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJx337shnKOi",
        "outputId": "28c0caf9-038e-4759-ab58-7b4ffb59a4c9"
      },
      "outputs": [],
      "source": [
        "rag_chain_from_docs = (\n",
        "    RunnablePassthrough.assign(\n",
        "        context=lambda x: format_docs(x[\"context\"])\n",
        "    )\n",
        "    | prompt_template\n",
        "    | groq_chat\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "retriever = db.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 4, \"fetch_k\": 10})\n",
        "\n",
        "print(\"Retreiver initialized successfully!\")\n",
        "\n",
        "rag_chain_with_source = RunnableParallel(\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        ").assign(answer=rag_chain_from_docs)\n",
        "\n",
        "chain = rag_chain_with_source\n",
        "print(\"RAG chain created successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "evalset = pd.read_excel(\"/kaggle/input/finaldataset/FinalRegNLP.xlsx\")\n",
        "evalset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install pandas openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "questions = []\n",
        "response_answers = []\n",
        "actual_answers = []\n",
        "\n",
        "for i in range(len(evalset)):\n",
        "    user_query = evalset['Question'][i]\n",
        "    response = chain.invoke(user_query)\n",
        "\n",
        "    answer = response[\"result\"]\n",
        "\n",
        "    questions.append(user_query)\n",
        "    response_answers.append(answer)\n",
        "    actual_answers.append(evalset['Answer'][i])\n",
        "\n",
        "print(\"Completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Number of questions:\", len(questions))\n",
        "print(\"Number of generated responses:\", len(response_answers))\n",
        "print(\"Number of actual answers:\", len(actual_answers))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if len(questions) == len(response_answers) == len(actual_answers):\n",
        "    new_df = pd.DataFrame({\n",
        "        'Question': questions,\n",
        "        'Generated Response': response_answers,\n",
        "        'Answer': actual_answers,\n",
        "    })\n",
        "\n",
        "    new_df.to_excel('LegalRAG_v1.xlsx', index=False, engine='openpyxl')\n",
        "    print(\"Results saved successfully!\")\n",
        "else:\n",
        "    print(\"Error: Lengths of the lists do not match. Please check the data.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "new_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
